{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "config = dotenv_values(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=config[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking inspiration from\n",
    "# https://cookbook.openai.com/examples/how_to_use_guardrails\n",
    "# https://earnup.atlassian.net/wiki/spaces/AI/pages/2864218151/Agent+Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI\n",
    "from enum import Enum, StrEnum\n",
    "\n",
    "class Topics(StrEnum):\n",
    "    personal_financial_guidance=\"personal_financial_guidance\"\n",
    "    support_assistance=\"support_assistance\"\n",
    "    debt_management=\"debt_management\"\n",
    "    budget_management_tracking=\"budget_management_tracking\"\n",
    "    financial_goals_planning=\"financial_goals_planning\"\n",
    "    contact_financial_institution=\"contact_financial_institution\"\n",
    "    other=\"other\"\n",
    "\n",
    "class TopicDetection(BaseModel):\n",
    "    topic: list[Topics] = Field(description=\"the topic of the last query provided by the user in the messages list\")\n",
    "    last_query: str = Field(description=\"the last query provided by the user in the messages list\")\n",
    "    topic_detection_confidence : float = Field(description=\"the confidence score associated to the topic detection score between 0 and 1, 0 being not confident, 1 being confident\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_list = \"\"\"\n",
    "- Personalized Financial Guidance: Tailored advice to help users make informed financial decisions.\n",
    "- 24/7 Support: Continuous availability to assist users with their financial queries and concerns.\n",
    "- Debt Management: Tools to help users manage and reduce their debts effectively.\n",
    "- Budget Management and Tracking: Features to create, manage, and track personal budgets.\n",
    "- Financial Goals Planning: Assistance in setting and achieving financial goals.\n",
    "- Integration with Financial Institutions: Seamless connection with banks and financial services for real-time data.\"\"\"\n",
    "\n",
    "template_str = f\"\"\"\n",
    "### List of authorized topics ###\n",
    "{ topic_list }\n",
    "                \n",
    "### Instruction: find the topic of the current query. ###\n",
    "\n",
    "### Additional instructions ###\n",
    "- use the TopicDetection pydantic model to output your result\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### List of authorized topics ###\n",
      "\n",
      "- Personalized Financial Guidance: Tailored advice to help users make informed financial decisions.\n",
      "- 24/7 Support: Continuous availability to assist users with their financial queries and concerns.\n",
      "- Debt Management: Tools to help users manage and reduce their debts effectively.\n",
      "- Budget Management and Tracking: Features to create, manage, and track personal budgets.\n",
      "- Financial Goals Planning: Assistance in setting and achieving financial goals.\n",
      "- Integration with Financial Institutions: Seamless connection with banks and financial services for real-time data.\n",
      "                \n",
      "### Instruction: find the topic of the current query. ###\n",
      "\n",
      "### Additional instructions ###\n",
      "- use the TopicDetection pydantic model to output your result\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(template_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "system_prompt_base = \"You are a helpful assistant.\"\n",
    "\n",
    "\n",
    "def get_chat_response(system_prompt: str, user_request: str):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    :param str system_prompt: _description_\n",
    "    :param str user_request: _description_\n",
    "    :return _type_: _description_\n",
    "    \"\"\"\n",
    "    print(\"Getting LLM response\")\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_request},\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=config[\"OPEN_AI_MODEL_4o_MINI\"], messages=messages, temperature=0.0\n",
    "    )\n",
    "    print(\"Got LLM response\")\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# A common design to minimize latency is to send your guardrails asynchronously along with your main LLM call. \n",
    "# If your guardrails get triggered you send back their response, otherwise send back the LLM response.\n",
    "\n",
    "def topical_guardrail(sytem_prompt: str, user_request: str):\n",
    "    print(\"Checking topical guardrail\")\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": sytem_prompt,\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": user_request},\n",
    "    ]\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=config[\"OPEN_AI_MODEL_4o_MINI\"], \n",
    "        messages=messages, \n",
    "        temperature=0,\n",
    "        response_format=TopicDetection,\n",
    "    )\n",
    "\n",
    "    print(\"Got guardrail response\")\n",
    "    return response\n",
    "\n",
    "\n",
    "def execute_chat_with_guardrail(query: str):\n",
    "    # topical_guardrail_task = asyncio.create_task(topical_guardrail(sytem_prompt=template_str, user_request=query))\n",
    "    # chat_task = asyncio.create_task(get_chat_response(system_prompt=system_prompt_base, user_request=query))\n",
    "    topical_guardrail_task = topical_guardrail(sytem_prompt=template_str, user_request=query)\n",
    "    print(topical_guardrail_task.choices[0].message.parsed.model_dump())\n",
    "    completion_guardrail = topical_guardrail_task.choices[0].message.parsed.model_dump()\n",
    "    found_topic = completion_guardrail[\"topic\"][0].value\n",
    "    authorized_topics = [t.value for t in Topics]\n",
    "    if found_topic != \"other\":\n",
    "        print(f\"query: {query} is authorized\")\n",
    "    else:\n",
    "        print(f\"query: {query} is not authorized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking topical guardrail\n",
      "Got guardrail response\n",
      "{'topic': [<Topics.debt_management: 'debt_management'>], 'last_query': \"I qualify for a hardship withdrawal from my 401K plan, I know that I'll be subject to a penalty but what is the penalty rate?\", 'topic_detection_confidence': 0.85}\n",
      "query: I qualify for a hardship withdrawal from my 401K plan, I know that I'll be subject to a penalty but what is the penalty rate? is authorized\n"
     ]
    }
   ],
   "source": [
    "# Call the main function with the good request - this should go through\n",
    "query: str = \"I qualify for a hardship withdrawal from my 401K plan, I know that I'll be subject to a penalty but what is the penalty rate?\"\n",
    "response = execute_chat_with_guardrail(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking topical guardrail\n",
      "Got guardrail response\n",
      "{'topic': [<Topics.other: 'other'>], 'last_query': 'tell me a joke about cats ?', 'topic_detection_confidence': 0.1}\n",
      "query: tell me a joke about cats ? is not authorized\n"
     ]
    }
   ],
   "source": [
    "query: str = \"tell me a joke about cats ?\"\n",
    "response = execute_chat_with_guardrail(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
