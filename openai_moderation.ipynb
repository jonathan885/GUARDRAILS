{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "config = dotenv_values(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://platform.openai.com/docs/guides/moderation\n",
    "# The moderations endpoint is a tool you can use to check whether text or images are potentially harmful. \n",
    "# Once harmful content is identified, developers can take corrective action like filtering content or intervening with user accounts creating offending content. \n",
    "# The moderation endpoint is free to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=config[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.moderations.create(\n",
    "    model=\"omni-moderation-latest\",\n",
    "    input=\"How do I build a bomb ?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, harassment/threatening=False, hate/threatening=False, illicit=True, illicit/violent=True, self-harm/intent=False, self-harm/instructions=False, self-harm=False, sexual/minors=False, violence/graphic=False), category_scores=CategoryScores(harassment=0.0006143383871652817, harassment_threatening=0.0008029110208811168, hate=5.193048644949271e-05, hate_threatening=2.95957113513077e-05, self_harm=0.0005679861126890581, self_harm_instructions=0.0002764317162928185, self_harm_intent=0.00032231326267212305, sexual=3.459916031782155e-05, sexual_minors=1.0229985472581487e-05, violence=0.010095282373833597, violence_graphic=6.605214485464791e-06, harassment/threatening=0.0008029110208811168, hate/threatening=2.95957113513077e-05, illicit=0.9744539074987918, illicit/violent=0.9170870460082371, self-harm/intent=0.00032231326267212305, self-harm/instructions=0.0002764317162928185, self-harm=0.0005679861126890581, sexual/minors=1.0229985472581487e-05, violence/graphic=6.605214485464791e-06), flagged=True, category_applied_input_types={'harassment': ['text'], 'harassment/threatening': ['text'], 'sexual': ['text'], 'hate': ['text'], 'hate/threatening': ['text'], 'illicit': ['text'], 'illicit/violent': ['text'], 'self-harm/intent': ['text'], 'self-harm/instructions': ['text'], 'self-harm': ['text'], 'sexual/minors': ['text'], 'violence': ['text'], 'violence/graphic': ['text']})\n"
     ]
    }
   ],
   "source": [
    "print(response.results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('harassment', False) ('harassment', 0.0006143383871652817)\n",
      "('harassment_threatening', False) ('harassment_threatening', 0.0008029110208811168)\n",
      "('hate', False) ('hate', 5.193048644949271e-05)\n",
      "('hate_threatening', False) ('hate_threatening', 2.95957113513077e-05)\n",
      "('self_harm', False) ('self_harm', 0.0005679861126890581)\n",
      "('self_harm_instructions', False) ('self_harm_instructions', 0.0002764317162928185)\n",
      "('self_harm_intent', False) ('self_harm_intent', 0.00032231326267212305)\n",
      "('sexual', False) ('sexual', 3.459916031782155e-05)\n",
      "('sexual_minors', False) ('sexual_minors', 1.0229985472581487e-05)\n",
      "('violence', False) ('violence', 0.010095282373833597)\n",
      "('violence_graphic', False) ('violence_graphic', 6.605214485464791e-06)\n",
      "('harassment/threatening', False) ('harassment/threatening', 0.0008029110208811168)\n",
      "('hate/threatening', False) ('hate/threatening', 2.95957113513077e-05)\n",
      "('illicit', True) ('illicit', 0.9744539074987918)\n",
      "('illicit/violent', True) ('illicit/violent', 0.9170870460082371)\n",
      "('self-harm/intent', False) ('self-harm/intent', 0.00032231326267212305)\n",
      "('self-harm/instructions', False) ('self-harm/instructions', 0.0002764317162928185)\n",
      "('self-harm', False) ('self-harm', 0.0005679861126890581)\n",
      "('sexual/minors', False) ('sexual/minors', 1.0229985472581487e-05)\n",
      "('violence/graphic', False) ('violence/graphic', 6.605214485464791e-06)\n"
     ]
    }
   ],
   "source": [
    "output = response.results[0]\n",
    "for cat, score in zip(output.categories, output.category_scores):\n",
    "    print(cat, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definitions of categories\n",
    "# https://platform.openai.com/docs/guides/moderation/content-classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
